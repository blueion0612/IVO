#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Real-time two-stage IMU gesture recognition with WebSocket integration
ì›ë³¸ realtime_two_stage_inference.pyì™€ ì™„ì „ ë™ì¼í•œ ë¡œì§ êµ¬í˜„
- 50Hz ë¦¬ìƒ˜í”Œë§
- Stage1 í›„ 2.5ì´ˆê°„ Stage2 candidate windows ìˆ˜ì§‘
- ê°€ì¥ ë†’ì€ confidenceì˜ ê²°ê³¼ ì„ íƒ
"""

import argparse
import socket
import struct
import time
import json
import asyncio
import threading
import os
import sys
from collections import deque
from typing import Tuple, Optional, Dict, Any

# Windows ì¸ì½”ë”© ì„¤ì •
if sys.platform == "win32":
    import io
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F

try:
    import websockets
except ImportError:
    print("[ERROR] websockets not installed. Please run: pip install websockets")
    sys.exit(1)

# -------------------------------------------------------------------
# Config (IMU Streaming App v0.4.1 - 30 floats)
# -------------------------------------------------------------------
WATCH_PHONE_IMU_LOOKUP = {
    # Watch ë°ì´í„° (ì¸ë±ìŠ¤ 0-14)
    "sw_dt": 0,           # ìƒ˜í”Œ ê°„ ì‹œê°„ ê°„ê²© (ì´ˆ)
    "sw_h": 1,            # íƒ€ì„ìŠ¤íƒ¬í”„ - ì‹œ
    "sw_m": 2,            # íƒ€ì„ìŠ¤íƒ¬í”„ - ë¶„
    "sw_s": 3,            # íƒ€ì„ìŠ¤íƒ¬í”„ - ì´ˆ
    "sw_ns": 4,           # íƒ€ì„ìŠ¤íƒ¬í”„ - ë‚˜ë…¸ì´ˆ
    # linear acceleration (m/sÂ²)
    "sw_lacc_x": 5, "sw_lacc_y": 6, "sw_lacc_z": 7,
    # gyroscope (rad/s)
    "sw_gyro_x": 8, "sw_gyro_y": 9, "sw_gyro_z": 10,
    # rotation vector (quaternion)
    "sw_rotvec_w": 11, "sw_rotvec_x": 12, "sw_rotvec_y": 13, "sw_rotvec_z": 14,

    # Phone ë°ì´í„° (ì¸ë±ìŠ¤ 15-29)
    "ph_dt": 15,          # ìƒ˜í”Œ ê°„ ì‹œê°„ ê°„ê²© (ì´ˆ)
    "ph_h": 16,           # íƒ€ì„ìŠ¤íƒ¬í”„ - ì‹œ
    "ph_m": 17,           # íƒ€ì„ìŠ¤íƒ¬í”„ - ë¶„
    "ph_s": 18,           # íƒ€ì„ìŠ¤íƒ¬í”„ - ì´ˆ
    "ph_ns": 19,          # íƒ€ì„ìŠ¤íƒ¬í”„ - ë‚˜ë…¸ì´ˆ
    # linear acceleration (m/sÂ²)
    "ph_lacc_x": 20, "ph_lacc_y": 21, "ph_lacc_z": 22,
    # gyroscope (rad/s)
    "ph_gyro_x": 23, "ph_gyro_y": 24, "ph_gyro_z": 25,
    # rotation vector (quaternion)
    "ph_rotvec_w": 26, "ph_rotvec_x": 27, "ph_rotvec_y": 28, "ph_rotvec_z": 29,
}

MSG_SIZE = 30 * 4  # 120 bytes (30 floats Ã— 4 bytes)
DEFAULT_PORT = 65000
HAPTIC_PORT = 65010  # í–…í‹± í”¼ë“œë°± ì „ì†¡ í¬íŠ¸

# -------------------------------------------------------------------
# Haptic Feedback Presets
# -------------------------------------------------------------------
HAPTIC_PRESETS = {
    # Stage1 ê°ì§€: ì§§ê³  ì•½í•œ ì§„ë™ 1íšŒ (ì¤€ë¹„ ì‹ í˜¸)
    "stage1_detected": {"intensity": 100, "count": 1, "duration": 80},

    # Stage2 ì¸ì‹ ì„±ê³µ: ê°•í•œ ì§„ë™ 2íšŒ (ì„±ê³µ í”¼ë“œë°±)
    "gesture_success": {"intensity": 255, "count": 2, "duration": 100},

    # Stage2 ì¸ì‹ ì‹¤íŒ¨: ì•½í•œ ì§„ë™ 3íšŒ ë¹ ë¥´ê²Œ (ì‹¤íŒ¨ í”¼ë“œë°±)
    "gesture_fail": {"intensity": 150, "count": 3, "duration": 50},

    # Drawing ëª¨ë“œ ì§„ì…: ì¤‘ê°„ ê°•ë„ 1íšŒ
    "mode_drawing": {"intensity": 180, "count": 1, "duration": 120},

    # Pointer ëª¨ë“œ ì§„ì…: ì•½í•œ ì§„ë™ 1íšŒ
    "mode_pointer": {"intensity": 120, "count": 1, "duration": 80},

    # ìƒ‰ìƒ/íŒ”ë ˆíŠ¸ ì„ íƒ: ì•„ì£¼ ì§§ì€ í‹±
    "selection_tick": {"intensity": 80, "count": 1, "duration": 50},

    # ìŠ¬ë¼ì´ë“œ ì´ë™: ì¤‘ê°„ ê°•ë„ 1íšŒ
    "slide_change": {"intensity": 150, "count": 1, "duration": 80},

    # ìº˜ë¦¬ë¸Œë ˆì´ì…˜ í¬ì¸íŠ¸ ê¸°ë¡: ì§§ì€ í”¼ë“œë°±
    "calibration_point": {"intensity": 200, "count": 1, "duration": 60},

    # ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ì™„ë£Œ: ì„±ê³µ íŒ¨í„´
    "calibration_done": {"intensity": 255, "count": 2, "duration": 150},

    # ë…¹ìŒ ì‹œì‘/ì¤‘ì§€: ì•Œë¦¼ ì§„ë™
    "recording_toggle": {"intensity": 200, "count": 1, "duration": 100},

    # OCR ì‹œì‘: ì§§ì€ ì•Œë¦¼
    "ocr_start": {"intensity": 150, "count": 1, "duration": 80},

    # OCR ì™„ë£Œ: ì„±ê³µ í”¼ë“œë°±
    "ocr_complete": {"intensity": 220, "count": 2, "duration": 80},
}

DETECTION_CHANNELS = [
    "sw_lacc_x", "sw_lacc_y", "sw_lacc_z",
    "sw_gyro_x", "sw_gyro_y", "sw_gyro_z",
]

# ì œìŠ¤ì²˜ ë§¤í•‘ (ê¸°ë³¸ê°’, configì—ì„œ ë®ì–´ì“°ê¸° ê°€ëŠ¥)
DEFAULT_GESTURE_TO_COMMAND = {
    0: "3",  # left -> Previous slide
    1: "4",  # right -> Next slide  
    2: "0",  # up -> Overlay ON
    3: "1",  # down -> Overlay OFF
    4: "5",  # circle_cw -> Caption start
    5: "8",  # circle_ccw -> Caption stop
    6: "JUMP_BACK",  # double_left -> Jump -3
    7: "JUMP_FORWARD",  # double_right -> Jump +3
    8: "2",  # x -> Reset all
    9: "6",  # double_tap -> Hand drawing
    10: "COLOR_PREV",  # 90_left -> Previous color
    11: "COLOR_NEXT",  # 90_right -> Next color
    12: "TIMER_TOGGLE",  # figure_eight -> Timer toggle
    13: "CALIBRATE",  # square -> Calibration
    14: "BLACKOUT"  # triangle -> Blackout
}

# Config íŒŒì¼ ë¡œë“œ í•¨ìˆ˜
def load_config(config_path="config.json"):
    """config.json íŒŒì¼ ë¡œë“œ"""
    if os.path.exists(config_path):
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            print(f"[CONFIG] Failed to load {config_path}: {e}")
    return {}

# ì „ì—­ ì„¤ì •
CONFIG = {}
GESTURE_TO_COMMAND = DEFAULT_GESTURE_TO_COMMAND.copy()

# -------------------------------------------------------------------
# Haptic Feedback Sender
# -------------------------------------------------------------------
class HapticSender:
    """UDPë¥¼ í†µí•´ í–…í‹± í”¼ë“œë°±ì„ Phoneìœ¼ë¡œ ì „ì†¡"""
    def __init__(self):
        self._socket = None
        self.phone_ip = None  # IMU íŒ¨í‚·ì—ì„œ ìë™ ê°ì§€

    def _get_socket(self):
        """ì†Œì¼“ íšë“ (í•„ìš”ì‹œ ìƒˆë¡œ ìƒì„±)"""
        if self._socket is None:
            self._socket = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
        return self._socket

    def set_phone_ip(self, ip: str):
        """Phone IP ì„¤ì • (IMU íŒ¨í‚· ìˆ˜ì‹  ì‹œ ìë™ í˜¸ì¶œ)"""
        if self.phone_ip != ip:
            self.phone_ip = ip
            print(f"[HAPTIC] Phone IP set to: {ip}")

    def send(self, preset_name: str = None, intensity: int = None, count: int = None, duration: int = None):
        """
        í–…í‹± í”¼ë“œë°± ì „ì†¡
        preset_name: HAPTIC_PRESETSì˜ í‚¤ (ì˜ˆ: "stage1_detected")
        ë˜ëŠ” ê°œë³„ íŒŒë¼ë¯¸í„°ë¡œ ì§ì ‘ ì§€ì •
        """
        if not self.phone_ip:
            print("[HAPTIC] Phone IP not set, skipping haptic")
            return False

        # í”„ë¦¬ì…‹ ì‚¬ìš©
        if preset_name and preset_name in HAPTIC_PRESETS:
            preset = HAPTIC_PRESETS[preset_name]
            intensity = preset["intensity"]
            count = preset["count"]
            duration = preset["duration"]
        elif intensity is None or count is None or duration is None:
            print(f"[HAPTIC] Invalid parameters: preset={preset_name}, i={intensity}, c={count}, d={duration}")
            return False

        # ë²”ìœ„ ê²€ì¦
        intensity = max(1, min(255, intensity))
        count = max(1, min(10, count))
        duration = max(50, min(500, duration))

        try:
            # Little Endianìœ¼ë¡œ íŒ¨í‚¹ (Python ê¸°ë³¸ê°’)
            data = struct.pack('<iii', intensity, count, duration)
            sock = self._get_socket()
            sock.sendto(data, (self.phone_ip, HAPTIC_PORT))
            print(f"[HAPTIC] Sent to {self.phone_ip}:{HAPTIC_PORT} - intensity={intensity}, count={count}, duration={duration}ms")
            return True
        except OSError as e:
            # ì†Œì¼“ ì˜¤ë¥˜ ì‹œ ì¬ìƒì„± ì‹œë„
            print(f"[HAPTIC] Socket error: {e}, recreating socket...")
            self._socket = None
            try:
                sock = self._get_socket()
                sock.sendto(data, (self.phone_ip, HAPTIC_PORT))
                print(f"[HAPTIC] Retry sent to {self.phone_ip}:{HAPTIC_PORT}")
                return True
            except Exception as e2:
                print(f"[HAPTIC] Retry failed: {e2}")
                return False
        except Exception as e:
            print(f"[HAPTIC] Send error: {e}")
            return False

    def close(self):
        if self._socket:
            try:
                self._socket.close()
            except:
                pass
            self._socket = None


# -------------------------------------------------------------------
# Haptic Receiver (ë³„ë„ WebSocket ì—°ê²°ë¡œ í–…í‹± ìš”ì²­ ìˆ˜ì‹ )
# ë™ê¸° ë°©ì‹ìœ¼ë¡œ êµ¬í˜„ - imu_test.pyì™€ ìœ ì‚¬í•˜ê²Œ
# -------------------------------------------------------------------
class HapticReceiver:
    """ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ ë™ê¸° WebSocketìœ¼ë¡œ í–…í‹± ìš”ì²­ ìˆ˜ì‹ """
    def __init__(self, ws_url: str, haptic_sender: HapticSender):
        self.ws_url = ws_url
        self.haptic_sender = haptic_sender
        self._thread = None
        self._stop = False

    def start(self):
        """ìˆ˜ì‹  ìŠ¤ë ˆë“œ ì‹œì‘"""
        self._stop = False
        self._thread = threading.Thread(target=self._run, daemon=True)
        self._thread.start()
        print(f"[HAPTIC-RX] Receiver thread started for {self.ws_url}", flush=True)

    def _run(self):
        """ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ ë™ê¸° WebSocket ì—°ê²°"""
        import websocket  # websocket-client ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš©

        while not self._stop:
            ws = None
            try:
                print(f"[HAPTIC-RX] Connecting to {self.ws_url}...", flush=True)
                ws = websocket.create_connection(self.ws_url, timeout=5)
                print(f"[HAPTIC-RX] Connected!", flush=True)

                ws.settimeout(0.5)  # recv íƒ€ì„ì•„ì›ƒ ì„¤ì •

                while not self._stop:
                    try:
                        message = ws.recv()
                        self._handle_message(message)
                    except websocket.WebSocketTimeoutException:
                        continue
                    except websocket.WebSocketConnectionClosedException:
                        print("[HAPTIC-RX] Connection closed, reconnecting...", flush=True)
                        break

            except Exception as e:
                if not self._stop:
                    print(f"[HAPTIC-RX] Connection error: {e}, retrying in 2s...", flush=True)
                    time.sleep(2.0)
            finally:
                if ws:
                    try:
                        ws.close()
                    except:
                        pass

    def _handle_message(self, raw_msg: str):
        """ìˆ˜ì‹  ë©”ì‹œì§€ ì²˜ë¦¬"""
        try:
            msg = json.loads(raw_msg)
            if msg.get("type") == "haptic_request":
                preset = msg.get("preset")
                print(f"[HAPTIC-RX] â˜… Haptic request: {preset}", flush=True)
                if preset and self.haptic_sender:
                    result = self.haptic_sender.send(preset)
                    print(f"[HAPTIC-RX] Send result: {result}", flush=True)
        except json.JSONDecodeError as e:
            print(f"[HAPTIC-RX] JSON decode error: {e}", flush=True)
        except Exception as e:
            print(f"[HAPTIC-RX] Handle error: {e}", flush=True)

    def stop(self):
        """ìˆ˜ì‹  ìŠ¤ë ˆë“œ ì¢…ë£Œ"""
        self._stop = True
        if self._thread and self._thread.is_alive():
            self._thread.join(timeout=2.0)


# -------------------------------------------------------------------
# WebSocket Command Sender
# -------------------------------------------------------------------
class CommandSender:
    def __init__(self, ws_url="ws://127.0.0.1:17890"):
        self.ws_url = ws_url
        self.ws = None
        self.connected = False

    async def connect(self):
        """WebSocket ì—°ê²°"""
        try:
            self.ws = await websockets.connect(self.ws_url)
            self.connected = True
            print(f"[WS] Connected to {self.ws_url}")
            return True
        except Exception as e:
            print(f"[WS] Failed to connect: {e}")
            self.connected = False
            return False

    async def send_message(self, message: dict):
        """ë©”ì‹œì§€ ì „ì†¡"""
        if not self.connected or not self.ws:
            if not await self.connect():
                return

        try:
            msg_str = json.dumps(message)
            await self.ws.send(msg_str)
        except Exception as e:
            print(f"[WS] Send error: {e}")
            self.connected = False
            await self.connect()

    async def send_stage1_detected(self, duration: float):
        """Stage1 ê°ì§€ ì•Œë¦¼"""
        await self.send_message({
            "type": "stage1_detected",
            "duration": duration
        })

    async def send_gesture_recognized(self, gesture_name: str, confidence: float):
        """ì œìŠ¤ì²˜ ì¸ì‹ ê²°ê³¼"""
        await self.send_message({
            "type": "gesture_recognized",
            "gesture": gesture_name,
            "confidence": confidence
        })

    async def send_command(self, cmd: str):
        """ëª…ë ¹ ì „ì†¡"""
        await self.send_message({"code": cmd})

    async def send_hold_extended(self, remaining_sec: float):
        """Hold ìƒíƒœ - Stage2 íƒ€ì´ë¨¸ ì—°ì¥ ì•Œë¦¼"""
        await self.send_message({
            "type": "hold_extended",
            "remaining": remaining_sec
        })

    async def send_stage2_cancelled(self):
        """Stage2 ì·¨ì†Œ ì•Œë¦¼ (hold í›„ ì›€ì§ì„ ì—†ì´ íƒ€ì„ì•„ì›ƒ)"""
        await self.send_message({
            "type": "stage2_cancelled"
        })

    async def close(self):
        if self.ws:
            await self.ws.close()
            self.connected = False

# -------------------------------------------------------------------
# IMU Listener (UDP) - ì›ë³¸ê³¼ ë™ì¼
# -------------------------------------------------------------------
class IMUListener:
    def __init__(self, ip: str, port: int):
        self.ip = ip
        self.port = port
        self.socket = None
        self.listening = False

    def start(self) -> bool:
        try:
            self.socket = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
            self.socket.bind((self.ip, self.port))
            self.socket.settimeout(0.1)
            self.listening = True
            print(f"[IMU] Listening on {self.ip}:{self.port}")
            return True
        except Exception as e:
            print(f"[IMU] Failed to bind {self.ip}:{self.port} - {e}")
            return False

    def stop(self):
        self.listening = False
        if self.socket:
            self.socket.close()
            self.socket = None
        print("[IMU] Listener stopped")

    def recv_one(self, timeout: float = 0.1) -> Optional[Tuple[float, np.ndarray, str]]:
        """
        IMU íŒ¨í‚· ìˆ˜ì‹ 
        Returns: (timestamp, values, sender_ip) ë˜ëŠ” None
        """
        if not self.listening or self.socket is None:
            return None
        self.socket.settimeout(timeout)
        try:
            data, addr = self.socket.recvfrom(MSG_SIZE)
            if len(data) != MSG_SIZE:
                return None
            values = struct.unpack('>30f', data)  # Big Endian, 30 floats
            ts = time.time()
            sender_ip = addr[0]  # Phone IP ì¶”ì¶œ
            return ts, np.array(values, dtype=np.float32), sender_ip
        except socket.timeout:
            return None
        except Exception as e:
            print(f"[IMU] Error receiving: {e}")
            return None

# -------------------------------------------------------------------
# Ring buffer - ì›ë³¸ê³¼ ë™ì¼
# -------------------------------------------------------------------
class IMURingBuffer:
    def __init__(self, maxlen: int):
        self.timestamps = deque(maxlen=maxlen)
        self.frames = deque(maxlen=maxlen)

    def add(self, timestamp: float, values: np.ndarray):
        self.timestamps.append(float(timestamp))
        self.frames.append(values.astype(np.float32))

    def __len__(self):
        return len(self.frames)

    def get_recent(self, n: int) -> Tuple[np.ndarray, np.ndarray]:
        n = min(n, len(self.frames))
        if n == 0:
            return np.zeros((0,), dtype=np.float64), np.zeros((0, 30), dtype=np.float32)
        times = np.array(list(self.timestamps)[-n:], dtype=np.float64)
        frames = np.stack(list(self.frames)[-n:], axis=0)
        return times, frames

    def get_by_time_range(self, t_start: float, t_end: float) -> Tuple[np.ndarray, np.ndarray]:
        times_list = []
        frames_list = []
        for ts, fr in zip(self.timestamps, self.frames):
            if ts < t_start or ts > t_end:
                continue
            times_list.append(ts)
            frames_list.append(fr)
        if not times_list:
            return np.zeros((0,), dtype=np.float64), np.zeros((0, 30), dtype=np.float32)
        return (
            np.array(times_list, dtype=np.float64),
            np.stack(frames_list, axis=0).astype(np.float32),
        )

    def clear(self):
        self.timestamps.clear()
        self.frames.clear()

# -------------------------------------------------------------------
# Model definitions - Stage1
# -------------------------------------------------------------------
class Stage1MLPModel(nn.Module):
    def __init__(self, input_shape):
        super().__init__()
        T, D = input_shape
        self.fc = nn.Sequential(
            nn.Flatten(),
            nn.Linear(T * D, 256),
            nn.ReLU(),
            nn.Dropout(0.4),
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Linear(64, 1),
        )

    def forward(self, x):
        return self.fc(x).squeeze(1)

class Stage1LSTMModel(nn.Module):
    def __init__(self, input_shape):
        super().__init__()
        T, D = input_shape
        self.lstm = nn.LSTM(
            input_size=D,
            hidden_size=64,
            num_layers=1,
            batch_first=True,
        )
        self.fc = nn.Sequential(
            nn.Dropout(0.4),
            nn.Linear(64, 64),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(64, 1),
        )

    def forward(self, x):
        _, (h_n, _) = self.lstm(x)
        h = h_n[-1]
        return self.fc(h).squeeze(1)

class Stage1GRUModel(nn.Module):
    def __init__(self, input_shape):
        super().__init__()
        T, D = input_shape
        self.gru = nn.GRU(
            input_size=D,
            hidden_size=64,
            num_layers=1,
            batch_first=True,
        )
        self.fc = nn.Sequential(
            nn.Dropout(0.4),
            nn.Linear(64, 64),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(64, 1),
        )

    def forward(self, x):
        _, h_n = self.gru(x)
        h = h_n[-1]
        return self.fc(h).squeeze(1)

def build_stage1_model(model_type: str, input_shape: tuple) -> nn.Module:
    if model_type == "mlp":
        return Stage1MLPModel(input_shape)
    elif model_type == "lstm":
        return Stage1LSTMModel(input_shape)
    elif model_type == "gru":
        return Stage1GRUModel(input_shape)
    else:
        print(f"[WARNING] Unknown Stage1 model_type '{model_type}', using MLP")
        return Stage1MLPModel(input_shape)

# -------------------------------------------------------------------
# Model definitions - Stage2
# -------------------------------------------------------------------
class Stage2MLPModel(nn.Module):
    def __init__(self, input_shape, num_classes):
        super().__init__()
        T, D = input_shape
        self.net = nn.Sequential(
            nn.Flatten(),
            nn.Linear(T * D, 256),
            nn.ReLU(),
            nn.Dropout(0.4),
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Linear(64, num_classes),
        )

    def forward(self, x):
        return self.net(x)

class Stage2GRUModel(nn.Module):
    def __init__(self, input_shape, num_classes):
        super().__init__()
        T, D = input_shape
        self.gru = nn.GRU(
            input_size=D,
            hidden_size=128,
            num_layers=2,
            batch_first=True,
            dropout=0.2,
        )
        self.fc = nn.Sequential(
            nn.Dropout(0.4),
            nn.Linear(128, 128),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(128, num_classes),
        )

    def forward(self, x):
        _, h_n = self.gru(x)
        h = h_n[-1]
        return self.fc(h)

# TCN components for Stage2
class TCNBlock(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size=3, dilation=1, dropout=0.2):
        super().__init__()
        padding = dilation * (kernel_size - 1)
        self.conv = nn.Conv1d(
            in_channels=in_channels,
            out_channels=out_channels,
            kernel_size=kernel_size,
            dilation=dilation,
            padding=padding,
        )
        self.bn = nn.BatchNorm1d(out_channels)
        self.dropout = nn.Dropout(dropout)
        self.relu = nn.ReLU()

        self.downsample = None
        if in_channels != out_channels:
            self.downsample = nn.Conv1d(in_channels, out_channels, kernel_size=1)

    def forward(self, x):
        out = self.conv(x)
        out = out[..., : x.size(-1)]
        out = self.bn(out)
        out = self.dropout(out)

        res = x
        if self.downsample is not None:
            res = self.downsample(x)

        out = self.relu(out + res)
        return out

class Stage2TCNModel(nn.Module):
    def __init__(self, input_shape, num_classes):
        super().__init__()
        T, D = input_shape
        C_in = D
        self.tcn = nn.Sequential(
            TCNBlock(C_in, 64, kernel_size=3, dilation=1, dropout=0.2),
            TCNBlock(64, 64, kernel_size=3, dilation=2, dropout=0.2),
            TCNBlock(64, 64, kernel_size=3, dilation=4, dropout=0.2),
        )
        self.pool = nn.AdaptiveAvgPool1d(1)
        self.fc = nn.Sequential(
            nn.Linear(64, 64),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(64, num_classes),
        )

    def forward(self, x):
        x = x.permute(0, 2, 1)
        out = self.tcn(x)
        out = self.pool(out).squeeze(-1)
        return self.fc(out)

def build_stage2_model(model_type: str, input_shape: tuple, num_classes: int) -> nn.Module:
    if model_type == "mlp":
        return Stage2MLPModel(input_shape, num_classes)
    elif model_type == "gru":
        return Stage2GRUModel(input_shape, num_classes)
    elif model_type == "tcn":
        return Stage2TCNModel(input_shape, num_classes)
    else:
        print(f"[WARNING] Unknown Stage2 model_type '{model_type}', using GRU")
        return Stage2GRUModel(input_shape, num_classes)

# -------------------------------------------------------------------
# Stage1 Detector (ì›ë³¸ê³¼ ë™ì¼í•œ ë¦¬ìƒ˜í”Œë§ ë¡œì§)
# -------------------------------------------------------------------
class Stage1Detector:
    def __init__(self, ckpt_path: str, buffer: IMURingBuffer, device: torch.device):
        self.buffer = buffer
        self.device = device
        self.last_infer_time = None

        ckpt: Dict[str, Any] = torch.load(ckpt_path, map_location=device)
        self.model_type: str = ckpt["model_type"]
        self.input_shape = tuple(ckpt["input_shape"])  # (T1, 6)
        self.window_sec: float = float(ckpt["window_sec"])
        self.step_sec: float = float(ckpt["step_sec"])
        self.threshold: float = float(ckpt["threshold"])
        self.target_fs: float = float(ckpt.get("target_fs", 50.0))

        self.detection_channels = ckpt.get("detection_channels", DETECTION_CHANNELS)
        self.det_indices = [WATCH_PHONE_IMU_LOOKUP[name] for name in self.detection_channels]

        self.mean = np.array(ckpt["norm_mean"], dtype=np.float32)
        self.std = np.array(ckpt["norm_std"], dtype=np.float32)
        self.eps = float(ckpt.get("norm_eps", 1e-6))
        self.clip_value = float(ckpt.get("norm_clip_value", 1e4))

        self.model = build_stage1_model(self.model_type, self.input_shape).to(device)
        
        # state_dict key handling
        if "state_dict" in ckpt:
            self.model.load_state_dict(ckpt["state_dict"])
        elif "model_state_dict" in ckpt:
            self.model.load_state_dict(ckpt["model_state_dict"])
        else:
            self.model.load_state_dict(ckpt)
            
        self.model.eval()

        print(f"[Stage1] Loaded from {ckpt_path}")
        print(f"  model_type = {self.model_type}")
        print(f"  window_sec = {self.window_sec}, step_sec = {self.step_sec}")
        print(f"  threshold  = {self.threshold}, target_fs = {self.target_fs}")

    def _preprocess(self, X: np.ndarray) -> np.ndarray:
        x = X.astype(np.float64)
        x = np.nan_to_num(x, nan=0.0, posinf=self.clip_value, neginf=-self.clip_value)
        x = np.clip(x, -self.clip_value, self.clip_value)
        std_safe = np.where(self.std < self.eps, 1.0, self.std)
        x = (x - self.mean) / std_safe
        x = np.nan_to_num(x, nan=0.0, posinf=0.0, neginf=0.0)
        return x.astype(np.float32)

    def maybe_detect(self, current_time: float) -> Tuple[bool, Optional[float]]:
        win_len = self.input_shape[0]  # T1
        if len(self.buffer) < win_len:
            return False, None

        # step_sec ê¸°ì¤€ìœ¼ë¡œ í˜¸ì¶œ ë¹ˆë„ ì œí•œ
        if (self.last_infer_time is not None) and (current_time - self.last_infer_time < self.step_sec):
            return False, None

        # 1) ì‹œê°„ êµ¬ê°„ ì„¤ì • (í•™ìŠµê³¼ ë™ì¼í•˜ê²Œ window_sec ê¸°ì¤€)
        t_end = current_time
        t_start = t_end - self.window_sec

        # 2) í•´ë‹¹ ì‹œê°„ êµ¬ê°„ì˜ ë°ì´í„°ë¥¼ ë²„í¼ì—ì„œ ê°€ì ¸ì˜¤ê¸°
        times, frames = self.buffer.get_by_time_range(t_start, t_end)
        if times.shape[0] < 2:
            return False, None

        # 3) ì„ íƒí•œ 6ì±„ë„ë§Œ ì¶”ì¶œ
        imu_raw = frames[:, self.det_indices]  # (N_raw, 6)

        # 4) 50Hzë¡œ ë¦¬ìƒ˜í”Œ (ê¸¸ì´ = win_len)
        dt = 1.0 / self.target_fs
        t_grid = t_start + np.arange(win_len, dtype=np.float64) * dt

        X_resampled = np.zeros((win_len, len(self.det_indices)), dtype=np.float32)
        for ch in range(len(self.det_indices)):
            X_resampled[:, ch] = np.interp(t_grid, times, imu_raw[:, ch])

        # 5) í•™ìŠµ ë•Œì™€ ë™ì¼í•œ z-score ì •ê·œí™”
        X_norm = self._preprocess(X_resampled)
        x_t = torch.from_numpy(X_norm[None, ...]).to(self.device)

        # 6) ì¶”ë¡ 
        with torch.no_grad():
            logits = self.model(x_t)
            if logits.ndim > 1:
                logits = logits.squeeze(1)
            prob = torch.sigmoid(logits)[0].item()

        self.last_infer_time = current_time
        is_gesture = prob >= self.threshold
        return bool(is_gesture), float(prob)

# -------------------------------------------------------------------
# Stage2 Classifier (ì›ë³¸ê³¼ ë™ì¼í•œ ë¦¬ìƒ˜í”Œë§ ë° í›„ë³´ ì„ íƒ ë¡œì§)
# -------------------------------------------------------------------
class Stage2Classifier:
    def __init__(self, ckpt_path: str, buffer: IMURingBuffer, device: torch.device):
        self.buffer = buffer
        self.device = device

        ckpt: Dict[str, Any] = torch.load(ckpt_path, map_location=device)
        self.model_type: str = ckpt["model_type"]
        self.input_shape = tuple(ckpt["input_shape"])  # (T2, 6)
        self.seq_len: int = int(ckpt.get("seq_len", self.input_shape[0]))
        self.num_classes: int = int(ckpt["num_classes"])
        self.class_id_to_name: Dict[int, str] = ckpt.get("class_id_to_name", {})

        self.detection_channels = ckpt.get("detection_channels", DETECTION_CHANNELS)
        self.det_indices = [WATCH_PHONE_IMU_LOOKUP[name] for name in self.detection_channels]

        self.mean = np.array(ckpt["norm_mean"], dtype=np.float32)
        self.std = np.array(ckpt["norm_std"], dtype=np.float32)
        self.eps = float(ckpt.get("norm_eps", 1e-6))
        self.clip_value = float(ckpt.get("norm_clip_value", 1e4))

        self.target_fs: float = float(ckpt.get("target_fs", 50.0))

        self.model = build_stage2_model(self.model_type, self.input_shape, self.num_classes).to(device)
        
        # state_dict key handling
        if "state_dict" in ckpt:
            self.model.load_state_dict(ckpt["state_dict"])
        elif "model_state_dict" in ckpt:
            self.model.load_state_dict(ckpt["model_state_dict"])
        else:
            self.model.load_state_dict(ckpt)
            
        self.model.eval()

        print(f"[Stage2] Loaded from {ckpt_path}")
        print(f"  model_type = {self.model_type}")
        print(f"  seq_len    = {self.seq_len}, num_classes = {self.num_classes}")

    @staticmethod
    def _center_crop_or_pad(seq: np.ndarray, target_len: int) -> np.ndarray:
        L, D = seq.shape
        if L == target_len:
            return seq.astype(np.float32)
        if L > target_len:
            start = (L - target_len) // 2
            end = start + target_len
            return seq[start:end].astype(np.float32)
        out = np.zeros((target_len, D), dtype=np.float32)
        start = (target_len - L) // 2
        out[start:start+L] = seq.astype(np.float32)
        return out

    def _preprocess(self, X: np.ndarray) -> np.ndarray:
        x = X.astype(np.float64)
        x = np.nan_to_num(x, nan=0.0, posinf=self.clip_value, neginf=-self.clip_value)
        x = np.clip(x, -self.clip_value, self.clip_value)
        std_safe = np.where(self.std < self.eps, 1.0, self.std)
        x = (x - self.mean) / std_safe
        x = np.nan_to_num(x, nan=0.0, posinf=0.0, neginf=0.0)
        return x.astype(np.float32)

    def classify_in_time_range(
        self,
        t_start: float,
        t_end: float,
        step_sec: float = 0.5,
        target_fs: float = 50.0,
    ) -> Tuple[Optional[int], Optional[str], Optional[float]]:

        times, frames = self.buffer.get_by_time_range(t_start, t_end)
        if frames.shape[0] == 0 or times.shape[0] < 2:
            return None, None, None

        imu_raw = frames[:, self.det_indices]  # (N_raw, 6)

        # 1) 50Hz ë¦¬ìƒ˜í”Œë§
        dt = 1.0 / target_fs
        duration = max(t_end - t_start, 0.0)
        resampled_len = int(round(duration * target_fs))
        if resampled_len < 2:
            return None, None, None

        t_grid = t_start + np.arange(resampled_len, dtype=np.float64) * dt
        imu_res = np.zeros((resampled_len, imu_raw.shape[1]), dtype=np.float32)
        for ch in range(imu_raw.shape[1]):
            imu_res[:, ch] = np.interp(t_grid, times, imu_raw[:, ch])

        N = imu_res.shape[0]
        win_len = self.seq_len

        # 2) ë¦¬ìƒ˜í”Œëœ ê¸¸ì´ê°€ seq_lenë³´ë‹¤ ì§§ìœ¼ë©´ -> center crop/pad í•œ ë²ˆë§Œ ìˆ˜í–‰
        if N < win_len:
            seq = self._center_crop_or_pad(imu_res, win_len)
            Xn = self._preprocess(seq)
            x_t = torch.from_numpy(Xn[None, ...]).to(self.device)
            with torch.no_grad():
                logits = self.model(x_t)
                probs = F.softmax(logits, dim=1).cpu().numpy()[0]
            best_id = int(np.argmax(probs))
            best_prob = float(probs[best_id])
            name = self.class_id_to_name.get(best_id, f"class_{best_id}")
            return best_id, name, best_prob

        # 3) ì¶©ë¶„íˆ ê¸¸ë©´: ë¦¬ìƒ˜í”Œëœ ì‹œí€€ìŠ¤ì—ì„œ ìŠ¬ë¼ì´ë”© ìœˆë„ìš°
        step_frames = max(1, int(round(step_sec * target_fs)))

        best_prob = -1.0
        best_id = None

        for start_idx in range(0, N - win_len + 1, step_frames):
            seg = imu_res[start_idx:start_idx + win_len]
            Xn = self._preprocess(seg)
            x_t = torch.from_numpy(Xn[None, ...]).to(self.device)
            with torch.no_grad():
                logits = self.model(x_t)
                probs = F.softmax(logits, dim=1).cpu().numpy()[0]
            p_max = float(probs.max())
            pred_id = int(np.argmax(probs))

            if p_max > best_prob:
                best_prob = p_max
                best_id = pred_id

        if best_id is None:
            return None, None, None

        name = self.class_id_to_name.get(best_id, f"class_{best_id}")
        return best_id, name, best_prob

# -------------------------------------------------------------------
# Main async loop (ì›ë³¸ê³¼ ë™ì¼í•œ ë¡œì§ + WebSocket í†µí•©)
# -------------------------------------------------------------------
async def main_async():
    parser = argparse.ArgumentParser(
        description="Real-time two-stage IMU gesture recognition with WebSocket"
    )
    parser.add_argument("--config", type=str, default="config.json",
                        help="Config file path (optional)")
    parser.add_argument("--ip", type=str, default=None,
                        help="Local IP to bind UDP socket")
    parser.add_argument("--port", type=int, default=None,
                        help="UDP port")
    parser.add_argument("--stage1_ckpt", type=str, default=None,
                        help="Path to Stage1 checkpoint")
    parser.add_argument("--stage2_ckpt", type=str, default=None,
                        help="Path to Stage2 checkpoint")
    parser.add_argument("--cooldown", type=float, default=None,
                        help="Seconds to ignore new Stage1 detections after one entry")
    parser.add_argument("--stage2_collect_sec", type=float, default=None,
                        help="Seconds after Stage1 to collect Stage2 candidate windows")
    parser.add_argument("--stage2_step_sec", type=float, default=None,
                        help="Stage2 candidate window step in seconds")
    parser.add_argument("--device", type=str, default=None,
                        help="cpu / cuda / auto")
    parser.add_argument("--ws_url", type=str, default=None,
                        help="WebSocket URL for sending commands")

    args = parser.parse_args()
    
    # Load config if exists
    global CONFIG, GESTURE_TO_COMMAND
    CONFIG = {}
    if os.path.exists(args.config):
        try:
            with open(args.config, 'r', encoding='utf-8') as f:
                CONFIG = json.load(f)
            print(f"[CONFIG] Loaded from {args.config}")
            
            # Update GESTURE_TO_COMMAND from config
            if "gesture_mapping" in CONFIG:
                for k, v in CONFIG["gesture_mapping"].items():
                    GESTURE_TO_COMMAND[int(k)] = v
                print(f"[CONFIG] Loaded {len(CONFIG['gesture_mapping'])} gesture mappings")
        except Exception as e:
            print(f"[CONFIG] Failed to load {args.config}: {e}")
            CONFIG = {}
    
    # Apply config with command line override
    imu_config = CONFIG.get("imu", {})
    ws_config = CONFIG.get("websocket", {})
    
    ip = args.ip or imu_config.get("udp_ip", "0.0.0.0")
    port = args.port or imu_config.get("udp_port", DEFAULT_PORT)
    stage1_ckpt = args.stage1_ckpt or imu_config.get("stage1_checkpoint", "./models/stage1_best.pt")
    stage2_ckpt = args.stage2_ckpt or imu_config.get("stage2_checkpoint", "./models/stage2_best.pt")
    cooldown = args.cooldown or imu_config.get("cooldown_sec", 2.0)
    stage2_collect_sec = args.stage2_collect_sec or imu_config.get("stage2_collection_sec", 2.5)
    stage2_step_sec = args.stage2_step_sec or imu_config.get("stage2_step_sec", 0.5)
    device_str = args.device or imu_config.get("device", "auto")
    ws_url = args.ws_url or ws_config.get("url", "ws://127.0.0.1:17890")

    # Device
    if device_str == "auto":
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    else:
        device = torch.device(device_str)
    print(f"[DEVICE] Using device: {device}")

    # Check checkpoint files
    if not os.path.exists(stage1_ckpt):
        print(f"[ERROR] Stage1 checkpoint not found: {stage1_ckpt}")
        return
    if not os.path.exists(stage2_ckpt):
        print(f"[ERROR] Stage2 checkpoint not found: {stage2_ckpt}")
        return

    # Stage1/Stage2 ckptë¥¼ ì ê¹ ì—´ì–´ì„œ window/seq_len, target_fs í™•ì¸
    tmp1 = torch.load(stage1_ckpt, map_location="cpu")
    T1 = int(tmp1["input_shape"][0])
    target_fs = float(tmp1.get("target_fs", 50.0))

    tmp2 = torch.load(stage2_ckpt, map_location="cpu")
    seq_len2 = int(tmp2.get("seq_len", tmp2["input_shape"][0]))

    max_T = max(T1, seq_len2)
    history_sec = max_T / target_fs * 4.0
    buffer_size = int(history_sec * target_fs)
    print(f"[BUFFER] history_secâ‰ˆ{history_sec:.1f}s, buffer_size={buffer_size}")

    imu_buffer = IMURingBuffer(maxlen=buffer_size)

    stage1 = Stage1Detector(stage1_ckpt, imu_buffer, device)
    stage2 = Stage2Classifier(stage2_ckpt, imu_buffer, device)

    # Haptic Feedback
    haptic_sender = HapticSender()

    # WebSocket (ëª…ë ¹ ì „ì†¡ìš©)
    cmd_sender = CommandSender(ws_url)
    await cmd_sender.connect()

    # Haptic Receiver (ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ í–…í‹± ìš”ì²­ ìˆ˜ì‹ )
    haptic_receiver = HapticReceiver(ws_url, haptic_sender)
    haptic_receiver.start()

    listener = IMUListener(ip, port)
    if not listener.start():
        return

    print("\n[RUN] Real-time two-stage inference with WebSocket started.")
    print("      Stage1: entry gesture detection")
    print(f"      Stage2: {stage2_collect_sec:.1f}s after entry, sliding windows step={stage2_step_sec:.2f}s")
    print("      Haptic feedback: enabled (receiver thread running)")
    print("      Press Ctrl+C to stop.\n")

    last_stage1_time = -1e9
    stage2_pending = False
    stage2_start_time = None

    # Hold detection settings
    # ê°€ì†ë„ + ìì´ë¡œ ëª¨ë‘ ê±°ì˜ ì•ˆ ì›€ì§ì¼ ë•Œë§Œ holdë¡œ íŒë‹¨
    HOLD_ACCEL_THRESHOLD = 0.3    # m/s^2 - ê°€ì†ë„ ë³€í™”ëŸ‰ ì„ê³„ê°’
    HOLD_GYRO_THRESHOLD = 0.15    # rad/s - ìì´ë¡œ ë³€í™”ëŸ‰ ì„ê³„ê°’ (ì•½ 8.6 deg/s)
    HOLD_EXTEND_SEC = 2.0         # hold ê°ì§€ ì‹œ ì—°ì¥í•  ì‹œê°„
    hold_check_interval = 0.5     # hold ì²´í¬ ê°„ê²© (ì´ˆ)
    last_hold_check_time = 0
    last_hold_notify_time = 0     # ë§ˆì§€ë§‰ hold ì•Œë¦¼ ì‹œê°„
    consecutive_hold_count = 0    # ì—°ì† hold ê°ì§€ íšŸìˆ˜ (2íšŒ ì´ìƒì¼ ë•Œë§Œ ì‹¤ì œ holdë¡œ íŒë‹¨)
    is_holding = False            # í˜„ì¬ hold ìƒíƒœì¸ì§€

    def calculate_motion_magnitude(buffer, window_sec=0.3):
        """
        ìµœê·¼ window_sec ë™ì•ˆì˜ ì›€ì§ì„ í¬ê¸° ê³„ì‚°
        ê°€ì†ë„(3ì¶•) + ìì´ë¡œ(3ì¶•) ì´ 6ì¶•ì˜ í‘œì¤€í¸ì°¨ ì‚¬ìš©
        """
        if len(buffer) < 5:
            return float('inf'), float('inf')  # ë°ì´í„° ë¶€ì¡±ì‹œ ì›€ì§ì„ ìˆëŠ” ê²ƒìœ¼ë¡œ ì²˜ë¦¬

        # ë²„í¼ì—ì„œ ìµœê·¼ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
        times, frames = buffer.get_recent(50)  # ìµœê·¼ 1ì´ˆ ë¶„ëŸ‰ (50Hz ê¸°ì¤€)
        if len(times) < 3:
            return float('inf'), float('inf')

        now = times[-1]
        t_start = now - window_sec

        # ì‹œê°„ ë²”ìœ„ ë‚´ ë°ì´í„°ë§Œ ì„ íƒ
        mask = times >= t_start
        if mask.sum() < 3:
            return float('inf'), float('inf')

        # Watch ê°€ì†ë„ ì¸ë±ìŠ¤: sw_lacc_x=5, sw_lacc_y=6, sw_lacc_z=7
        # Watch ìì´ë¡œ ì¸ë±ìŠ¤: sw_gyro_x=8, sw_gyro_y=9, sw_gyro_z=10
        accel_data = frames[mask][:, 5:8]
        gyro_data = frames[mask][:, 8:11]

        # ê°€ì†ë„ì˜ í‘œì¤€í¸ì°¨ (m/s^2)
        accel_std = np.std(accel_data, axis=0)
        accel_mag = np.linalg.norm(accel_std)

        # ìì´ë¡œì˜ í‘œì¤€í¸ì°¨ (rad/s)
        gyro_std = np.std(gyro_data, axis=0)
        gyro_mag = np.linalg.norm(gyro_std)

        return accel_mag, gyro_mag

    try:
        while True:
            pkt = listener.recv_one(timeout=0.01)

            if pkt is None:
                # Stage2 timeout ì²´í¬
                if stage2_pending:
                    now = time.time()
                    if now >= stage2_start_time + stage2_collect_sec:
                        pred_id, pred_name, best_prob = stage2.classify_in_time_range(
                            t_start=stage2_start_time,
                            t_end=stage2_start_time + stage2_collect_sec,
                            step_sec=stage2_step_sec,
                            target_fs=target_fs,
                        )

                        if pred_id is not None:
                            print(
                                f"[STAGE2] Final gesture (best of candidates): "
                                f"id={pred_id}, name={pred_name}, conf={best_prob:.3f}"
                            )

                            # Send gesture recognition event
                            await cmd_sender.send_gesture_recognized(pred_name, best_prob)

                            # Send command if mapped
                            if pred_id in GESTURE_TO_COMMAND:
                                cmd = GESTURE_TO_COMMAND[pred_id]
                                print(f"         â†’ Command: {cmd}")
                                await cmd_sender.send_command(cmd)

                            # í–…í‹± í”¼ë“œë°±: ì œìŠ¤ì²˜ ì¸ì‹ ì„±ê³µ
                            haptic_sender.send("gesture_success")
                        else:
                            print("[STAGE2] Not enough data or no valid candidate in window.")
                            # í–…í‹± í”¼ë“œë°±: ì œìŠ¤ì²˜ ì¸ì‹ ì‹¤íŒ¨
                            haptic_sender.send("gesture_fail")

                        # ğŸ” Stage2 ëë‚¬ìœ¼ë‹ˆ ë²„í¼/ìƒíƒœ ì´ˆê¸°í™” + ì¿¨ë‹¤ìš´
                        stage2_pending = False
                        imu_buffer.clear()
                        stage1.last_infer_time = None
                        last_stage1_time = time.time()

                await asyncio.sleep(0.001)
                continue

            ts, values, sender_ip = pkt
            imu_buffer.add(ts, values)

            # Phone IP ìë™ ì„¤ì • (í–…í‹± í”¼ë“œë°±ìš©)
            haptic_sender.set_phone_ip(sender_ip)

            # âœ… Stage2 ëŒ€ê¸° ì¤‘: Hold ê°ì§€ ë° íƒ€ì´ë¨¸ ì—°ì¥ ì²´í¬
            if stage2_pending:
                current_deadline = stage2_start_time + stage2_collect_sec

                # Hold ì²´í¬ (ì¼ì • ê°„ê²©ìœ¼ë¡œ)
                if ts - last_hold_check_time >= hold_check_interval:
                    last_hold_check_time = ts
                    accel_mag, gyro_mag = calculate_motion_magnitude(imu_buffer)

                    # ê°€ì†ë„ì™€ ìì´ë¡œ ëª¨ë‘ ì„ê³„ê°’ ì´í•˜ì¼ ë•Œë§Œ holdë¡œ íŒë‹¨
                    is_still = (accel_mag < HOLD_ACCEL_THRESHOLD) and (gyro_mag < HOLD_GYRO_THRESHOLD)

                    if is_still:
                        consecutive_hold_count += 1

                        # ì—°ì† 2íšŒ ì´ìƒ ì •ì§€ ê°ì§€ë  ë•Œë§Œ ì‹¤ì œ holdë¡œ íŒë‹¨
                        if consecutive_hold_count >= 2:
                            # ë¬´í•œ ëŒ€ê¸°: íƒ€ì´ë¨¸ ê³„ì† ì—°ì¥
                            stage2_collect_sec = ts - stage2_start_time + HOLD_EXTEND_SEC

                            # hold ìƒíƒœ ì‹œì‘ ì‹œ í•œë²ˆë§Œ ì•Œë¦¼
                            if not is_holding:
                                is_holding = True
                                print(f"[HOLD] Arm held still (accel={accel_mag:.3f}, gyro={gyro_mag:.3f}), waiting...")
                                await cmd_sender.send_hold_extended(-1)  # -1 = ë¬´í•œ ëŒ€ê¸°
                    else:
                        # ì›€ì§ì„ ê°ì§€ - ì—°ì† hold ì¹´ìš´íŠ¸ ë¦¬ì…‹
                        consecutive_hold_count = 0
                        is_holding = False

            # âœ… Stage2 ëŒ€ê¸° ì¤‘ì´ê³ , ì‹œê°„ì´ ì§€ë‚˜ë©´ ë°”ë¡œ Stage2 ìˆ˜í–‰
            if stage2_pending and ts >= stage2_start_time + stage2_collect_sec:
                pred_id, pred_name, best_prob = stage2.classify_in_time_range(
                    t_start=stage2_start_time,
                    t_end=stage2_start_time + stage2_collect_sec,
                    step_sec=stage2_step_sec,
                    target_fs=target_fs,
                )

                if pred_id is not None:
                    print(
                        f"[STAGE2] Final gesture (best of candidates): "
                        f"id={pred_id}, name={pred_name}, conf={best_prob:.3f}"
                    )

                    # Send gesture recognition event
                    await cmd_sender.send_gesture_recognized(pred_name, best_prob)

                    # Send command if mapped
                    if pred_id in GESTURE_TO_COMMAND:
                        cmd = GESTURE_TO_COMMAND[pred_id]
                        print(f"         â†’ Command: {cmd}")
                        await cmd_sender.send_command(cmd)

                    # í–…í‹± í”¼ë“œë°±: ì œìŠ¤ì²˜ ì¸ì‹ ì„±ê³µ
                    haptic_sender.send("gesture_success")
                else:
                    print("[STAGE2] Not enough data or no valid candidate in window.")
                    # í–…í‹± í”¼ë“œë°±: ì œìŠ¤ì²˜ ì¸ì‹ ì‹¤íŒ¨
                    haptic_sender.send("gesture_fail")

                # ğŸ” ì—¬ê¸°ì„œë„ ë™ì¼í•˜ê²Œ ë²„í¼/ìƒíƒœ ë¦¬ì…‹ + ì¿¨ë‹¤ìš´ ì‹œì‘
                stage2_pending = False
                imu_buffer.clear()
                stage1.last_infer_time = None
                last_stage1_time = time.time()
                # Hold ë³€ìˆ˜ ë¦¬ì…‹
                consecutive_hold_count = 0
                is_holding = False
                stage2_collect_sec = args.stage2_collect_sec or imu_config.get("stage2_collection_sec", 2.5)
                continue

            # ì•„ì§ Stage2 ëŒ€ê¸° ìƒíƒœê°€ ì•„ë‹ ë•Œë§Œ Stage1ìœ¼ë¡œ ì—”íŠ¸ë¦¬ ê°ì§€
            if not stage2_pending:
                is_gesture, prob = stage1.maybe_detect(ts)
                if not is_gesture:
                    continue

                # cooldown ì ìš© (ë„ˆë¬´ ìì£¼ ì—”íŠ¸ë¦¬ ê°ì§€ë˜ëŠ” ê²ƒ ë°©ì§€)
                if ts - last_stage1_time < cooldown:
                    continue

                # ğŸ”¥ ì—¬ê¸°ì„œ ë²„í¼/ìƒíƒœ ë¦¬ì…‹: ì•ìœ¼ë¡œ ë“¤ì–´ì˜¤ëŠ” ë°ì´í„°ëŠ” Stage2ìš©
                imu_buffer.clear()
                stage1.last_infer_time = None

                last_stage1_time = ts
                stage2_pending = True
                stage2_start_time = ts

                # Hold ë³€ìˆ˜ ë¦¬ì…‹
                consecutive_hold_count = 0
                is_holding = False
                last_hold_check_time = ts
                last_hold_notify_time = 0
                stage2_collect_sec = args.stage2_collect_sec or imu_config.get("stage2_collection_sec", 2.5)

                print(
                    f"[STAGE1] Entry gesture detected! prob={prob:.3f}\n"
                    f"          -> Perform Stage2 gesture within {stage2_collect_sec:.1f}s (hold to extend)..."
                )

                # Send Stage1 detection notification
                await cmd_sender.send_stage1_detected(stage2_collect_sec)

                # í–…í‹± í”¼ë“œë°±: Stage1 ê°ì§€ (ì¤€ë¹„ ì‹ í˜¸)
                haptic_sender.send("stage1_detected")

    except KeyboardInterrupt:
        print("\n[RUN] Interrupted by user.")
    except Exception as e:
        print(f"\n[ERROR] Unexpected error: {e}")
        import traceback
        traceback.print_exc()
    finally:
        listener.stop()
        haptic_receiver.stop()
        await cmd_sender.close()
        haptic_sender.close()
        print("[RUN] Finished.")

def main():
    """Entry point"""
    try:
        asyncio.run(main_async())
    except Exception as e:
        print(f"[ERROR] Failed to run: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()
